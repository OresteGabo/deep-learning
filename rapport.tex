\documentclass[12pt,a4paper]{article}

% --- Paquets standard ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{setspace}

\geometry{margin=2.5cm}
\onehalfspacing

% --- Configuration de l'affichage du code Python ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single
}

\newcommand{\hr}[1]{\rule{\linewidth}{#1}}

\begin{document}

% --- PAGE DE GARDE ---
\begin{titlepage}
    \begin{center}
        \includegraphics[width=0.4\textwidth]{assets/logo.png}\\[1cm]
        \textsc{\Large Université de Haute-Alsace}\\[0.5cm]
        \textsc{\large Faculté des Sciences et Techniques (FST)}\\[0.5cm]
        \textsc{\large Master 2 Informatique et Mobilité}\\[1.5cm]

        \hr{0.5pt}
        \vspace{0.4cm}
        {\huge \bfseries Projet Final : Apprentissage Profond} \\[0.4cm]
        {\Large Étude des architectures pour la prédiction d'événements sismologiques majeurs}
        \vspace{0.4cm}
        \hr{0.5pt}

        \vspace{2cm}

        \begin{minipage}{0.45\textwidth}
            \begin{flushleft} \large
                \emph{Auteur :}\\
                \textbf{Oreste MUHIRWA GABO}\\
                \texttt{orestegabo@icloud.com}
            \end{flushleft}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \begin{flushright} \large
                \emph{Enseignant :}\\
                M. Maxime DEVANNE \\
                Responsable Deep Learning
            \end{flushright}
        \end{minipage}
        \vfill
        {\large 16 Janvier 2025}
    \end{center}
\end{titlepage}

\newpage
\tableofcontents
\newpage

\section{Introduction}
Ce rapport présente une étude comparative d'architectures de réseaux de neurones profonds appliquées à la détection d'événements sismologiques majeurs. Le centre sismologique nécessite une automatisation capable de traiter plusieurs relevés par minute. En utilisant le jeu de données \textit{Earthquakes} (archive UCR), nous analysons des séries temporelles de 512 heures pour distinguer les signaux "normaux" des signes précurseurs d'un séisme.

\section{Méthodologie et Justification des Choix}
\subsection{Prétraitement et Régularisation}
La robustesse de l'évaluation repose sur une normalisation \textbf{Z-Score} systématisée pour chaque série temporelle. Cette étape est cruciale car elle permet au réseau de se focaliser sur les variations de fréquences et de formes d'ondes plutôt que sur l'amplitude brute du signal.
Pour contrer le surapprentissage, nous avons utilisé :
\begin{itemize}
    \item \textbf{Dropout (0.3) :} Utilisé dans le classifieur final pour désactiver aléatoirement des neurones, forçant le modèle à apprendre des représentations redondantes.
    \item \textbf{Batch Normalization :} Appliquée dans le CNN pour stabiliser le gradient et accélérer la convergence.
    \item \textbf{Weighted CrossEntropy Loss :} Un poids de 3.0 est attribué à la classe minoritaire pour pallier le déséquilibre (74.8\% de cas normaux).
\end{itemize}

\subsection{Optimisation et Reproductibilité}
Nous utilisons l'optimiseur \textbf{Adam} couplé au scheduler \textbf{OneCycleLR}. Ce dernier est justifié par sa capacité à effectuer une "super-convergence" en faisant varier le taux d'apprentissage de manière cyclique. Pour garantir la reproductibilité, une graine aléatoire fixe (\textit{Seed 42}) a été utilisée, bien que nous observions des variations mineures dues aux opérations non-déterministes de l'accélération matérielle MPS.

\section{Analyse des Architectures}
\subsection{Multi-Layer Perceptron (MLP)}
Le MLP sert de base de référence (Baseline). Il traite les 512 points comme un vecteur plat.
\textbf{Analyse des logs :} Avec une perte finale de 0.0001 mais une précision de test stagnante à 69.78\%, le modèle montre un surapprentissage clair. Il mémorise le bruit du train set sans généraliser la structure temporelle.

\subsection{Convolutional Neural Network (CNN)}
Le CNN utilise des noyaux 1D pour extraire des caractéristiques spatiales locales (pics d'activité).
\textbf{Analyse des logs :} Bien qu'il atteigne une \textit{Best Accuracy} de 75.54\%, il reste instable. Sa matrice de confusion montre qu'il peine à isoler les séismes réels (Rappel de 0.37).

\subsection{Recurrent Neural Network (RNN/LSTM)}
Le RNN est l'architecture la plus adaptée théoriquement aux séries temporelles grâce à ses cellules à mémoire.
\textbf{Analyse des logs :} Le RNN s'est révélé être le modèle le plus performant avec une précision record de \textbf{81.29\%} et un score F1 de \textbf{0.54} pour les séismes. Il identifie 21 séismes sur 35, un résultat significatif pour la sécurité sismologique.

\subsection{Modèle Hybride : SeismicNet}
Fusionnant CNN et Bi-LSTM, ce modèle vise à extraire des motifs locaux avant de les traiter séquentiellement.
\textbf{Analyse des logs :} Il atteint une précision de 78.42\% et se distingue par son temps d'inférence extrêmement optimisé (0.0045s).

\section{Étude de Complexité et Performance}
Conformément aux contraintes matérielles du centre sismologique, nous présentons le tableau comparatif suivant, exécuté sur architecture MacBook Pro M4 :

\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Modèle} & \textbf{Paramètres} & \textbf{Inférence (s)} & \textbf{Best Acc.} & \textbf{F1-Séis.} \\
        \midrule
        MLP & 74,050 & 0.0067s & 74.82\% & 0.25 \\
        CNN & \textbf{43,842} & 0.0088s & 75.54\% & 0.35 \\
        \textbf{RNN} & 50,562 & 0.0074s & \textbf{81.29\%} & \textbf{0.54} \\
        SeismicNet & 85,698 & \textbf{0.0045s} & 78.42\% & 0.37 \\
        \bottomrule
    \end{tabular}
    \caption{Comparaison technique des modèles entraînés sur 100 époques.}
\end{table}

\section{Discussion sur la Robustesse et Variabilité}
Il est important de noter que l'entraînement de réseaux profonds est un processus stochastique. Bien que la graine soit fixée, des exécutions répétées peuvent mener à des résultats légèrement différents (variations de $\pm 2\%$). Cette variabilité souligne la nécessité d'une évaluation robuste par matrice de confusion. Par exemple, le RNN présente un équilibre sain entre précision (0.49) et rappel (0.60) pour la classe Earthquake, ce qui est crucial pour minimiser les faux négatifs (séismes non détectés).

\section{Conclusion}
Le modèle **RNN** est recommandé pour sa capacité de détection supérieure (Meilleur Score F1). Toutefois, le **SeismicNet** représente l'avenir du déploiement opérationnel grâce à son temps d'inférence record, permettant de traiter des milliers de relevés par minute sur les machines modestes du service.

\newpage
\section{Annexes : Code et Implémentation}
\begin{lstlisting}[language=Python, caption=Architecture du SeismicNet Hybride]
class SeismicNet(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64):
        super().__init__()
        self.feature_extractor = nn.Sequential(
            nn.Conv1d(input_dim, 32, kernel_size=7, padding=3),
            nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),
            nn.Conv1d(32, 64, kernel_size=5, padding=2),
            nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2)
        )
        self.lstm = nn.LSTM(64, hidden_dim, bidirectional=True, batch_first=True)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64), nn.ReLU(),
            nn.Dropout(0.3), nn.Linear(64, 2)
        )

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.feature_extractor(x).transpose(1, 2)
        lstm_out, _ = self.lstm(x)
        return self.classifier(lstm_out[:, -1, :])
\end{lstlisting}

\end{document}